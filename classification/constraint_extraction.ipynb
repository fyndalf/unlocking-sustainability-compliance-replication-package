{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PIPELINE: 0) read 1) preprocess 2) query and parse response 3) analyse\n",
    "\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# define data structure for easier result parsing later on\n",
    "@dataclass\n",
    "class parsed_process_constraints:\n",
    "    control_flow: int\n",
    "    control_flow_in_activity: int\n",
    "    data: int\n",
    "    data_in_activity: int\n",
    "    time: int\n",
    "    time_in_activity: int\n",
    "    resource: int\n",
    "    resource_in_activity: int\n",
    "    no_process_constraint: int\n",
    "\n",
    "\n",
    "PATH_TO_PROMPT_TEMPLATE = \"../prompt_few_shot.txt\"  # replace with ../prompt_zero_shot.txt for a zero-shot approach.\n",
    "# https://github.com/ollama/ollama/blob/main/docs/api.md temperature and seed for reproducibility\n",
    "MODEL_TEMPERATURE = 0.1\n",
    "MODEL_SEED = 1312\n",
    "\n",
    "APPENDIX_A_REFERENCE = (\n",
    "    \"The activity complies with the criteria set out in Appendix A to this Annex.\"\n",
    ")\n",
    "APPENDIX_B_REFERENCE = (\n",
    "    \"The activity complies with the criteria set out in Appendix B to this Annex.\"\n",
    ")\n",
    "APPENDIX_C_REFERENCE = (\n",
    "    \"The activity complies with the criteria set out in Appendix C to this Annex.\"\n",
    ")\n",
    "APPENDIX_D_REFERENCE = (\n",
    "    \"The activity complies with the criteria set out in Appendix D to this Annex.\"\n",
    ")\n",
    "APPENDIX_E_REFERENCE = (\n",
    "    \"The activity complies with the criteria set out in Appendix E to this Annex.\"\n",
    ")\n",
    "\n",
    "SUBSTANTIAL_CONTRIBUTION_CRITERIA = \"Substantial contribution criteria\"\n",
    "DNSH_CLIMATE_ADAPTION = \"DNSH on Climate adaptation\"\n",
    "DNSH_CLIMATE_MITIGATION = \"DNSH on Climate mitigation\"\n",
    "DNSH_WATER = \"DNSH on Water\"\n",
    "DNSH_CIRCULAR_ECONOMY = \"DNSH on Circular economy\"\n",
    "DNSH_POLLUTION = \"DNSH on Pollution prevention\"\n",
    "DNSH_BIODIVERSITY = \"DNSH on Biodiversity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following prompting from https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Prompt_Engineering_with_Llama_3.ipynb\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Get a free API key from https://console.groq.com/keys, and put it into .env in the top level of this repo\n",
    "load_dotenv('../')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_KEY')\n",
    "\n",
    "# Defines the exact model to be used. Here, we use the one fine-tuned for following instructions.\n",
    "LLAMA3_70B_INSTRUCT = \"llama3-70b-8192\"\n",
    "LLAMA3_8B_INSTRUCT = \"llama3-8b-8192\"\n",
    "\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = LLAMA3_8B_INSTRUCT,\n",
    "    temperature: float = MODEL_TEMPERATURE, # for quasi-deterministic responses\n",
    "    top_p: float = 0.1, # for quasi-deterministic responses\n",
    "    seed = MODEL_SEED, # for quasi-deterministic responses\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return response.choices[0].message.content  \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = LLAMA3_8B_INSTRUCT,\n",
    "    temperature: float = MODEL_TEMPERATURE, # for quasi-deterministic responses\n",
    "    top_p: float = 0.1, # for quasi-deterministic responses\n",
    "    seed = MODEL_SEED, # for quasi-deterministic responses\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "#def complete_and_print(prompt: str, model: str = LLAMA3_8B_INSTRUCT):\n",
    "#    print(f'==============\\n{prompt}\\n==============')\n",
    "#    response = completion(prompt, model)\n",
    "#    print(response, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PIPELINE Stage 0: Read Files #####\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Parse environmental objective catalogues\n",
    "climate_mitigation = pd.read_excel('../taxonomy/taxonomy.xlsx', sheet_name=\"Climate mitigation\")\n",
    "climate_adaption = pd.read_excel('../taxonomy/taxonomy.xlsx', sheet_name=\"Climate adaptation\")\n",
    "water = pd.read_excel('../taxonomy/taxonomy.xlsx', sheet_name=\"Water\")\n",
    "circular_economy = pd.read_excel('../taxonomy/taxonomy.xlsx', sheet_name=\"Circular economy\")\n",
    "pollution_prevention = pd.read_excel('../taxonomy/taxonomy.xlsx', sheet_name=\"Pollution prevention\")\n",
    "biodiversity = pd.read_excel('../taxonomy/taxonomy.xlsx', sheet_name=\"Biodiversity\")\n",
    "\n",
    "# Read texts of taxonomy appendices\n",
    "appendix_a = Path(\"../taxonomy/appendix_a\").read_text()\n",
    "appendix_b = Path(\"../taxonomy/appendix_b\").read_text()\n",
    "appendix_c = Path(\"../taxonomy/appendix_c\").read_text()\n",
    "appendix_d = Path(\"../taxonomy/appendix_d\").read_text()\n",
    "appendix_e = Path(\"../taxonomy/appendix_e\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PIPELINE Stage 1: Preprocess Catalogues #####\n",
    "\n",
    "# Preprocess catalogues by 1) filtering out N/As, 2) re-adding footnotes\n",
    "\n",
    "from numpy import NaN\n",
    "import re\n",
    "\n",
    "criteria = [\n",
    "    SUBSTANTIAL_CONTRIBUTION_CRITERIA,\n",
    "    DNSH_CLIMATE_ADAPTION,\n",
    "    DNSH_CLIMATE_MITIGATION,\n",
    "    DNSH_WATER,\n",
    "    DNSH_CIRCULAR_ECONOMY,\n",
    "    DNSH_POLLUTION,\n",
    "    DNSH_BIODIVERSITY,\n",
    "]\n",
    "\n",
    "def filter_nan(s):\n",
    "    if s == \"N/A\" or s == \"N/A \":\n",
    "        return None\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "\n",
    "def preprocess_row(row):\n",
    "    # filter out N/A and replace with None\n",
    "    row[:] = row[:].apply(filter_nan)\n",
    "\n",
    "    if row[\"Footnotes\"] is not None and row[\"Footnotes\"] is not NaN:\n",
    "        # Add footnote to appropriate criteria chunks, should they contain footnotes\n",
    "        for single_criteria in criteria:\n",
    "            try:\n",
    "                if row[single_criteria] is not None and isinstance(\n",
    "                    row[single_criteria], str\n",
    "                ):\n",
    "                    footnotes_in_single_criteria = re.search(\n",
    "                        r\"\\(+[0-9]\\)\", row[single_criteria]\n",
    "                    )\n",
    "                    if footnotes_in_single_criteria is not None:\n",
    "                        # append footnotes to chunk\n",
    "                        row[single_criteria] = (\n",
    "                            row[single_criteria] + \"\\n\\nHere are additional footnotes for the text passage:\\n\" + row[\"Footnotes\"]\n",
    "                        )\n",
    "            except KeyError:\n",
    "                continue\n",
    "    return row\n",
    "\n",
    "\n",
    "climate_mitigation_preprocessed = climate_mitigation.apply(\n",
    "    lambda row: preprocess_row(row), axis=1\n",
    ")\n",
    "climate_adaption_preprocessed = climate_adaption.apply(\n",
    "    lambda row: preprocess_row(row), axis=1\n",
    ")\n",
    "water_preprocessed = water.apply(lambda row: preprocess_row(row), axis=1)\n",
    "circular_economy_preprocessed = circular_economy.apply(\n",
    "    lambda row: preprocess_row(row), axis=1\n",
    ")\n",
    "pollution_prevention_preprocessed = pollution_prevention.apply(\n",
    "    lambda row: preprocess_row(row), axis=1\n",
    ")\n",
    "biodiversity_preprocessed = biodiversity.apply(lambda row: preprocess_row(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PIPELINE Stage 2 + 3: Query LLM per catalogue, row, criteria (chunk) and parse response #####\n",
    "import time\n",
    "import ast\n",
    "from typing import Optional\n",
    "import regex\n",
    "from groq import RateLimitError, InternalServerError\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    retry_if_exception_type,\n",
    "    wait_exponential,\n",
    "    retry_if_result,\n",
    ")\n",
    "\n",
    "\n",
    "def log_before_prompt_retry(retry_state):\n",
    "    if retry_state.outcome.failed:\n",
    "        print(\n",
    "            \"retrying attempt \"\n",
    "            + str(retry_state.attempt_number)\n",
    "            + \", ended with exception:\"\n",
    "            + str(retry_state.outcome)\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"retrying attempt \"\n",
    "            + str(retry_state.attempt_number)\n",
    "            + \", ended with \"\n",
    "            + str(retry_state.outcome.result())\n",
    "        )\n",
    "\n",
    "\n",
    "def is_none_p(value):\n",
    "    \"\"\"Return True if process constraints value is None\"\"\"\n",
    "    return value[0] is None\n",
    "\n",
    "\n",
    "criteria = [\n",
    "    SUBSTANTIAL_CONTRIBUTION_CRITERIA,\n",
    "    DNSH_CLIMATE_ADAPTION,\n",
    "    DNSH_CLIMATE_MITIGATION,\n",
    "    DNSH_WATER,\n",
    "    DNSH_CIRCULAR_ECONOMY,\n",
    "    DNSH_POLLUTION,\n",
    "    DNSH_BIODIVERSITY,\n",
    "]\n",
    "\n",
    "APPENDIX_A_CRITERIA = None\n",
    "APPENDIX_A_RESPONSE = None\n",
    "APPENDIX_B_CRITERIA = None\n",
    "APPENDIX_B_RESPONSE = None\n",
    "APPENDIX_C_CRITERIA = None\n",
    "APPENDIX_C_RESPONSE = None\n",
    "APPENDIX_D_CRITERIA = None\n",
    "APPENDIX_D_RESPONSE = None\n",
    "APPENDIX_E_CRITERIA = None\n",
    "APPENDIX_E_RESPONSE = None\n",
    "\n",
    "# per catalogue: per substantial contribution, DNSH: query model with prompt template\n",
    "from pathlib import Path\n",
    "\n",
    "prompt_template = Path(PATH_TO_PROMPT_TEMPLATE).read_text()\n",
    "\n",
    "\n",
    "# Insert chunk of taxonomy into prepared prompt\n",
    "def create_prompt_from_preprocessed_chunk(\n",
    "    chunk: str, activity_description: Optional[str], template: str\n",
    "):\n",
    "    # we opt to not add references to existing standards or legislation here, as it's safe to assume (checked with some samples) that these texts are part of the LLM's training data.\n",
    "    # TODO: look at https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Prompt_Engineering_with_Llama_3.ipynb for better few-shot prompting with assistant messages\n",
    "\n",
    "    if activity_description is None:\n",
    "        return template.format(description_of_activity=\"\", taxonomy_chunk=chunk)\n",
    "    else:\n",
    "        description = (\n",
    "            \"\\nHere is a high-level description of the process that is regulated by the text passage you will be given later on:\\n\"\n",
    "            + activity_description\n",
    "        )\n",
    "        return template.format(\n",
    "            description_of_activity=description, taxonomy_chunk=chunk\n",
    "        )\n",
    "\n",
    "\n",
    "# Query model with prepared prompt, and parse response into constraints\n",
    "# Retry in case a rate limit has been hit or no constraints were present in the response, with an increasing waiting time\n",
    "@retry(\n",
    "    stop=stop_after_attempt(30),\n",
    "    retry=(\n",
    "        retry_if_exception_type(tuple((RateLimitError, InternalServerError)))\n",
    "        | retry_if_result(is_none_p)\n",
    "    ),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=900),\n",
    "    before_sleep=log_before_prompt_retry,\n",
    ")\n",
    "def query_and_parse_response_for_chunk_prompt(\n",
    "    chunk_prompt: str,\n",
    ") -> tuple[Optional[parsed_process_constraints], str]:\n",
    "    # print(chunk_prompt)\n",
    "\n",
    "    response = completion(chunk_prompt)\n",
    "\n",
    "    \"\"\" The response should contain an object-like string which we want to parse:\n",
    "    {\n",
    "    'control-flow': {\n",
    "        'within_activities': [number of activity existence constraints],\n",
    "        'between_activities': [number of control-flow constraints between activities]\n",
    "    },\n",
    "    'temporal': {\n",
    "        'within_activities': [number of temporal constraints within activities],\n",
    "        'between_activities': [number of temporal constraints between activities]\n",
    "    },\n",
    "    'resource': {\n",
    "        'within_activities': [number of resource constraints within activities],\n",
    "        'between_activities': [number of resource constraints between activities]\n",
    "    },\n",
    "    'data':{\n",
    "        'within_activities': [number of data constraints within activities],\n",
    "        'between_activities': [number of data constraints between activities]\n",
    "    },\n",
    "    'irrelevant': [number of irrelevant constraints]\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    matched_response = regex.search(r\"{((?:[^{}]|(?R))*)}\", response, regex.VERBOSE)\n",
    "    if matched_response is None or matched_response[0] == \"\":\n",
    "        print(\"Found no match!\")\n",
    "        print(response)\n",
    "        return None, response\n",
    "    # print(\"match! parsing now...\")\n",
    "    preprocessed_response = matched_response[0].replace(\n",
    "        \"//\", \"#\"\n",
    "    )  # replace comment-like symbol with an actual comment\n",
    "    preprocessed_response = preprocessed_response.replace(\n",
    "        \"[\", \"\"\n",
    "    )  # replace list-like symbol\n",
    "    preprocessed_response = preprocessed_response.replace(\n",
    "        \"]\", \"\"\n",
    "    )  # replace list-like symbol\n",
    "\n",
    "    parsed_response = ast.literal_eval(preprocessed_response)\n",
    "    return (\n",
    "        parsed_process_constraints(\n",
    "            control_flow=parsed_response[\"control-flow\"][\"between_activities\"],\n",
    "            control_flow_in_activity=parsed_response[\"control-flow\"][\n",
    "                \"within_activities\" # this is actually an activity existence constraint\n",
    "            ],\n",
    "            time=parsed_response[\"temporal\"][\"between_activities\"],\n",
    "            time_in_activity=parsed_response[\"control-flow\"][\"between_activities\"],\n",
    "            resource=parsed_response[\"resource\"][\"between_activities\"],\n",
    "            resource_in_activity=parsed_response[\"resource\"][\"within_activities\"],\n",
    "            data=parsed_response[\"data\"][\"between_activities\"],\n",
    "            data_in_activity=parsed_response[\"data\"][\"within_activities\"],\n",
    "            no_process_constraint=parsed_response[\"irrelevant\"],\n",
    "        ),\n",
    "        response,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create prompt for chunk and then prompt and return parsed result\n",
    "def prompt_for_criteria(\n",
    "    criteria: str, description: Optional[str]\n",
    ") -> tuple[parsed_process_constraints, str]:\n",
    "    # create prompt for a specific DNSH/contribution criteria\n",
    "\n",
    "    prompt = create_prompt_from_preprocessed_chunk(\n",
    "        criteria, description, prompt_template\n",
    "    )\n",
    "    # sleep 2 seconds to not hit rate limit of Groq\n",
    "    time.sleep(2)\n",
    "\n",
    "    # return parsed result of querying LLM. Prompt is re-done automatically until we get a proper response\n",
    "    return query_and_parse_response_for_chunk_prompt(prompt) # type: ignore\n",
    "    \n",
    "\n",
    "# Prompt model for a specific criteria of a taxonomy row\n",
    "def prompt_for_criteria_of_row_if_exists(\n",
    "    criteria: str, description: Optional[str], row\n",
    "):\n",
    "    try:\n",
    "        if row[criteria] is not None and row[criteria] is not NaN:\n",
    "            criteria_of_row: str = row[criteria]\n",
    "            # Use previously generated results for appendices, if they solely referenced\n",
    "            if criteria_of_row in APPENDIX_A_REFERENCE:\n",
    "                return APPENDIX_A_CRITERIA, APPENDIX_A_RESPONSE\n",
    "            elif criteria_of_row in APPENDIX_B_REFERENCE:\n",
    "                return APPENDIX_B_CRITERIA, APPENDIX_B_RESPONSE\n",
    "            elif criteria_of_row in APPENDIX_C_REFERENCE:\n",
    "                return APPENDIX_C_CRITERIA, APPENDIX_C_RESPONSE\n",
    "            elif criteria_of_row in APPENDIX_D_REFERENCE:\n",
    "                return APPENDIX_D_CRITERIA, APPENDIX_D_RESPONSE\n",
    "            elif criteria_of_row in APPENDIX_E_REFERENCE:\n",
    "                return APPENDIX_E_CRITERIA, APPENDIX_E_RESPONSE\n",
    "            # Otherwise, prompt model directly\n",
    "            else:\n",
    "                # if only partly referenced, add appendix to criteria\n",
    "                if \"Appendix A\" in criteria_of_row:\n",
    "                    criteria_of_row += \"\\n\\n Appendix A:\\n\" + appendix_a\n",
    "                if \"Appendix B\" in criteria_of_row:\n",
    "                    criteria_of_row += \"\\n\\n Appendix B:\\n\" + appendix_b\n",
    "                if \"Appendix C\" in criteria_of_row:\n",
    "                    criteria_of_row += \"\\n\\n Appendix C:\\n\" + appendix_c\n",
    "                if \"Appendix D\" in criteria_of_row:\n",
    "                    criteria_of_row += \"\\n\\n Appendix D:\\n\" + appendix_d\n",
    "                if \"Appendix E\" in criteria_of_row:\n",
    "                    criteria_of_row += \"\\n\\n Appendix E:\\n\" + appendix_e\n",
    "                constraints_of_criteria_row, raw_response = prompt_for_criteria(\n",
    "                    criteria_of_row, description=description\n",
    "                )\n",
    "        else:\n",
    "            constraints_of_criteria_row = None\n",
    "            raw_response = \"\"\n",
    "    except KeyError:\n",
    "        constraints_of_criteria_row = None\n",
    "        raw_response = \"\"\n",
    "    return constraints_of_criteria_row, raw_response\n",
    "\n",
    "\n",
    "def query_row_of_catalogue(goal: str, row):\n",
    "    description = row[\"Description\"]\n",
    "\n",
    "    substantial_contribution = row[SUBSTANTIAL_CONTRIBUTION_CRITERIA]\n",
    "    substantial_contribution_constraints, substantial_contribution_response = (\n",
    "        prompt_for_criteria(substantial_contribution, description)\n",
    "    )\n",
    "\n",
    "    climate_mitigation_dnsh_constraints, climate_mitigation_response = (\n",
    "        prompt_for_criteria_of_row_if_exists(DNSH_CLIMATE_MITIGATION, description, row)\n",
    "    )\n",
    "    climate_adaption_dnsh_constraints, climate_adaption_response = (\n",
    "        prompt_for_criteria_of_row_if_exists(DNSH_CLIMATE_ADAPTION, description, row)\n",
    "    )\n",
    "    water_dnsh_constraints, water_response = prompt_for_criteria_of_row_if_exists(\n",
    "        DNSH_WATER, description, row\n",
    "    )\n",
    "    circular_dnsh_constraints, circular_response = prompt_for_criteria_of_row_if_exists(\n",
    "        DNSH_CIRCULAR_ECONOMY, description, row\n",
    "    )\n",
    "    pollution_dnsh_constraints, pollution_response = (\n",
    "        prompt_for_criteria_of_row_if_exists(DNSH_POLLUTION, description, row)\n",
    "    )\n",
    "\n",
    "    biodiversity_dnsh_constraints, biodiversity_response = (\n",
    "        prompt_for_criteria_of_row_if_exists(DNSH_BIODIVERSITY, description, row)\n",
    "    )\n",
    "\n",
    "    if row[\"NACE\"] is not None and isinstance(row[\"NACE\"], str):\n",
    "        nace_codes = row[\"NACE\"].split(\",\")\n",
    "    else:\n",
    "        nace_codes = []\n",
    "\n",
    "    if row[\"Contribution type\"] is not None and row[\"Contribution type\"] is not NaN:\n",
    "        contribution_type = row[\"Contribution type\"]\n",
    "    else:\n",
    "        contribution_type = None\n",
    "\n",
    "    row[\"goal\"] = goal\n",
    "    row[\"nace\"] = nace_codes\n",
    "    row[\"sector\"] = row[\"Sector\"]\n",
    "    row[\"number\"] = row[\"Activity number\"]\n",
    "    row[\"activity\"] = row[\"Activity\"]\n",
    "    row[\"substantial\"] = substantial_contribution_constraints\n",
    "    row[\"mitigation\"] = climate_mitigation_dnsh_constraints\n",
    "    row[\"adaption\"] = climate_adaption_dnsh_constraints\n",
    "    row[\"water\"] = water_dnsh_constraints\n",
    "    row[\"circular\"] = circular_dnsh_constraints\n",
    "    row[\"pollution\"] = pollution_dnsh_constraints\n",
    "    row[\"biodiversity\"] = biodiversity_dnsh_constraints\n",
    "    row[\"contribution\"] = contribution_type\n",
    "    row[\"substantial_response\"] = substantial_contribution_response\n",
    "    row[\"mitigation_response\"] = climate_mitigation_response\n",
    "    row[\"adaption_response\"] = climate_adaption_response\n",
    "    row[\"water_response\"] = water_response\n",
    "    row[\"circular_response\"] = circular_response\n",
    "    row[\"pollution_response\"] = pollution_response\n",
    "    row[\"biodiversity_response\"] = biodiversity_response\n",
    "    return row[\n",
    "        [\n",
    "            \"number\",\n",
    "            \"nace\",\n",
    "            \"goal\",\n",
    "            \"sector\",\n",
    "            \"activity\",\n",
    "            \"substantial\",\n",
    "            \"mitigation\",\n",
    "            \"adaption\",\n",
    "            \"water\",\n",
    "            \"circular\",\n",
    "            \"pollution\",\n",
    "            \"biodiversity\",\n",
    "            \"contribution\",\n",
    "            \"substantial_response\",\n",
    "            \"mitigation_response\",\n",
    "            \"adaption_response\",\n",
    "            \"water_response\",\n",
    "            \"circular_response\",\n",
    "            \"pollution_response\",\n",
    "            \"biodiversity_response\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "# Prepare appendix chunks by prompting for them once\n",
    "APPENDIX_A_CRITERIA, APPENDIX_A_RESPONSE = prompt_for_criteria(appendix_a, \"Take into account all relevant and irrelevant constraints you can identify in the text below.\")\n",
    "APPENDIX_B_CRITERIA, APPENDIX_B_RESPONSE = prompt_for_criteria(appendix_b, \"Take into account all relevant and irrelevant constraints you can identify in the text below.\")\n",
    "APPENDIX_C_CRITERIA, APPENDIX_C_RESPONSE = prompt_for_criteria(appendix_c, \"Take into account all relevant and irrelevant constraints you can identify in the text below.\")\n",
    "APPENDIX_D_CRITERIA, APPENDIX_D_RESPONSE = prompt_for_criteria(appendix_d, \"Take into account all relevant and irrelevant constraints you can identify in the text below.\")\n",
    "APPENDIX_E_CRITERIA, APPENDIX_E_RESPONSE = prompt_for_criteria(appendix_e, \"Take into account all relevant and irrelevant constraints you can identify in the text below. In this case, technical specifications can be interpreted as data constraints of general activities that deal with water appliances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply prompt to all tables, all constraint cells of taxonomy\n",
    "\n",
    "\n",
    "def apply_prompt(df, goal):\n",
    "    df = df.apply(lambda row: query_row_of_catalogue(goal, row), axis=1)\n",
    "    return df\n",
    "\n",
    "water_parsed = apply_prompt(water_preprocessed, \"Water\")\n",
    "water_parsed.to_csv(\n",
    "    \"../output/water_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\", sep=\"|\"\n",
    ")\n",
    "\n",
    "circular_economy_parsed = apply_prompt(\n",
    "    circular_economy_preprocessed, \"Circular economy\"\n",
    ")\n",
    "circular_economy_parsed.to_csv(\n",
    "    \"../output/circular_economy_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\", sep=\"|\"\n",
    ")\n",
    "\n",
    "pollution_prevention_parsed = apply_prompt(\n",
    "    pollution_prevention_preprocessed, \"Pollution prevention\"\n",
    ")\n",
    "pollution_prevention_parsed.to_csv(\n",
    "    \"../output/pollution_prevention_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\", sep=\"|\"\n",
    ")\n",
    "\n",
    "biodiversity_parsed = apply_prompt(biodiversity_preprocessed, \"Biodiversity\")\n",
    "biodiversity_parsed.to_csv(\n",
    "    \"../output/biodiversity_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\", sep=\"|\"\n",
    ")\n",
    "\n",
    "climate_mitigation_parsed = apply_prompt(\n",
    "    climate_mitigation_preprocessed, \"Climate mitigation\"\n",
    ")\n",
    "climate_mitigation_parsed.to_csv(\n",
    "    \"../output/climate_mitigation_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\", sep=\"|\"\n",
    ")\n",
    "\n",
    "climate_adaption_parsed = apply_prompt(\n",
    "    climate_adaption_preprocessed, \"Climate adaption\"\n",
    ")\n",
    "climate_adaption_parsed.to_csv(\n",
    "    \"../output/climate_adaption_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\", sep=\"|\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each taxonomy data frame to have columns per contribution goal and constraint type\n",
    "\n",
    "\n",
    "def flatten_element(element):\n",
    "    # Make sure that we only have ints in there\n",
    "    if type(element) is list:\n",
    "        return int(element[0])\n",
    "    else:\n",
    "        return int(element)\n",
    "\n",
    "def flatten_contribution(row, criteria):\n",
    "    process_constraints: parsed_process_constraints = row[criteria]\n",
    "    # Expand parsed process constraints into single columns\n",
    "    if process_constraints is not None:\n",
    "        row[criteria + \"_control_flow_between\"] = flatten_element(\n",
    "            process_constraints.control_flow\n",
    "        )\n",
    "        row[criteria + \"_control_flow_within\"] = flatten_element(\n",
    "            process_constraints.control_flow_in_activity # this is actually referring to activity existence constraint\n",
    "        )\n",
    "        row[criteria + \"_temporal_between\"] = flatten_element(process_constraints.time)\n",
    "        row[criteria + \"_temporal_within\"] = flatten_element(\n",
    "            process_constraints.time_in_activity\n",
    "        )\n",
    "        row[criteria + \"_resource_between\"] = flatten_element(\n",
    "            process_constraints.resource\n",
    "        )\n",
    "        row[criteria + \"_resource_within\"] = flatten_element(\n",
    "            process_constraints.resource_in_activity\n",
    "        )\n",
    "        row[criteria + \"_data_between\"] = flatten_element(process_constraints.data)\n",
    "        row[criteria + \"_data_within\"] = flatten_element(\n",
    "            process_constraints.data_in_activity\n",
    "        )\n",
    "        row[criteria + \"_no_process_relation\"] = flatten_element(\n",
    "            process_constraints.no_process_constraint\n",
    "        )\n",
    "    else:\n",
    "        row[criteria + \"_control_flow_between\"] = 0\n",
    "        row[criteria + \"_control_flow_within\"] = 0 # this is actually referring to activity existence constraint\n",
    "        row[criteria + \"_temporal_between\"] = 0\n",
    "        row[criteria + \"_temporal_within\"] = 0\n",
    "        row[criteria + \"_resource_between\"] = 0\n",
    "        row[criteria + \"_resource_within\"] = 0\n",
    "        row[criteria + \"_data_between\"] = 0\n",
    "        row[criteria + \"_data_within\"] = 0\n",
    "        row[criteria + \"_no_process_relation\"] = 0\n",
    "    return row\n",
    "\n",
    "\n",
    "def flatten_row_of_taxonomy(row):\n",
    "    row = flatten_contribution(row, \"substantial\")\n",
    "    row = flatten_contribution(row, \"mitigation\")\n",
    "    row = flatten_contribution(row, \"adaption\")\n",
    "    row = flatten_contribution(row, \"water\")\n",
    "    row = flatten_contribution(row, \"circular\")\n",
    "    row = flatten_contribution(row, \"pollution\")\n",
    "    row = flatten_contribution(row, \"biodiversity\")\n",
    "    # Drop columns we do no longer need for direct analysis and have exported to csv in a previous step\n",
    "    row = row.drop(\n",
    "        [\n",
    "            \"substantial\",\n",
    "            \"mitigation\",\n",
    "            \"adaption\",\n",
    "            \"water\",\n",
    "            \"circular\",\n",
    "            \"pollution\",\n",
    "            \"biodiversity\",\n",
    "            \"contribution\",\n",
    "            \"substantial_response\",\n",
    "            \"mitigation_response\",\n",
    "            \"adaption_response\",\n",
    "            \"water_response\",\n",
    "            \"circular_response\",\n",
    "            \"pollution_response\",\n",
    "            \"biodiversity_response\"\n",
    "        ]\n",
    "    )\n",
    "    return row\n",
    "\n",
    "\n",
    "def flatten_taxonomy(df):\n",
    "    df = df.apply(lambda row: flatten_row_of_taxonomy(row), axis=1)\n",
    "    return df\n",
    "\n",
    "output_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "climate_mitigation_flattened = flatten_taxonomy(climate_mitigation_parsed)\n",
    "climate_mitigation_flattened.to_csv(\n",
    "    \"../output/climate_mitigation_flattened_\" + output_time + \".csv\",\n",
    "    sep=\"|\",\n",
    ")\n",
    "\n",
    "climate_adaption_flattened = flatten_taxonomy(climate_adaption_parsed)\n",
    "climate_adaption_flattened.to_csv(\n",
    "    \"../output/climate_adaption_flattened_\" + output_time + \".csv\",\n",
    "    sep=\"|\",\n",
    ")\n",
    "\n",
    "water_flattened = flatten_taxonomy(water_parsed)\n",
    "water_flattened.to_csv(\n",
    "    \"../output/water_flattened_\" + output_time + \".csv\", sep=\"|\"\n",
    ")\n",
    "\n",
    "circular_economy_flattened = flatten_taxonomy(circular_economy_parsed)\n",
    "circular_economy_flattened.to_csv(\n",
    "    \"../output/circular_economy_flattened_\" + output_time + \".csv\",\n",
    "    sep=\"|\",\n",
    ")\n",
    "\n",
    "pollution_prevention_flattened = flatten_taxonomy(pollution_prevention_parsed)\n",
    "pollution_prevention_flattened.to_csv(\n",
    "    \"../output/pollution_prevention_flattened_\"\n",
    "    + output_time\n",
    "    + \".csv\",\n",
    "    sep=\"|\",\n",
    ")\n",
    "\n",
    "biodiversity_flattened = flatten_taxonomy(biodiversity_parsed)\n",
    "biodiversity_flattened.to_csv(\n",
    "    \"../output/biodiversity_flattened_\" + output_time + \".csv\",\n",
    "    sep=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\164278636.py:3: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['nace', 'goal', 'sector', 'activity'], dtype='object')]\n",
      "\n",
      "  climate_mitigation_flattened.to_hdf('../output/climate_mitigation_flattened.h5', key='random_data')\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\164278636.py:4: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['nace', 'goal', 'sector', 'activity'], dtype='object')]\n",
      "\n",
      "  climate_adaption_flattened.to_hdf('../output/climate_adaption_flattened.h5', key='random_data')\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\164278636.py:5: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['nace', 'goal', 'sector', 'activity'], dtype='object')]\n",
      "\n",
      "  water_flattened.to_hdf('../output/water_flattened.h5', key='random_data')\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\164278636.py:6: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['nace', 'goal', 'sector', 'activity'], dtype='object')]\n",
      "\n",
      "  circular_economy_flattened.to_hdf('../output/circular_economy_flattened.h5',key='random_data')\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\164278636.py:7: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['nace', 'goal', 'sector', 'activity'], dtype='object')]\n",
      "\n",
      "  pollution_prevention_flattened.to_hdf('../output/pollution_prevention_flattened.h5',key='random_data')\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\164278636.py:8: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['nace', 'goal', 'sector', 'activity'], dtype='object')]\n",
      "\n",
      "  biodiversity_flattened.to_hdf('../output/biodiversity_flattened.h5',key='random_data')\n"
     ]
    }
   ],
   "source": [
    "# Make results available in second notebook for further analysis\n",
    "\n",
    "climate_mitigation_flattened.to_hdf('../output/climate_mitigation_flattened.h5', key='random_data')\n",
    "climate_adaption_flattened.to_hdf('../output/climate_adaption_flattened.h5', key='random_data')\n",
    "water_flattened.to_hdf('../output/water_flattened.h5', key='random_data')\n",
    "circular_economy_flattened.to_hdf('../output/circular_economy_flattened.h5',key='random_data')\n",
    "pollution_prevention_flattened.to_hdf('../output/pollution_prevention_flattened.h5',key='random_data')\n",
    "biodiversity_flattened.to_hdf('../output/biodiversity_flattened.h5',key='random_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:6: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  concatinated_taxonomy.to_excel(writer, \"Overall results\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:7: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  climate_mitigation_flattened.to_excel(writer, \"Climate mitigation\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:8: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  climate_adaption_flattened.to_excel(writer, \"Climate adaption\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:9: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  water_flattened.to_excel(writer, \"Water\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:10: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  circular_economy_flattened.to_excel(writer, \"Circular economy\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:11: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  pollution_prevention_flattened.to_excel(writer, \"Pollution prevention\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:12: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  biodiversity_flattened.to_excel(writer, \"Biodiversity\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:16: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  climate_mitigation_parsed.to_excel(writer, \"Climate mitigation\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:17: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  climate_adaption_parsed.to_excel(writer, \"Climate adaption\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:18: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  water_parsed.to_excel(writer, \"Water\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:19: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  circular_economy_parsed.to_excel(writer, \"Circular economy\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:20: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  pollution_prevention_parsed.to_excel(writer, \"Pollution prevention\")\n",
      "C:\\Users\\Finn\\AppData\\Local\\Temp\\ipykernel_10616\\2173629367.py:21: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
      "  biodiversity_parsed.to_excel(writer, \"Biodiversity\")\n"
     ]
    }
   ],
   "source": [
    "# Store each constraint set and their concatination in an excel, as well as the raw output for further inspection\n",
    "\n",
    "concatinated_taxonomy = pd.concat([climate_mitigation_flattened, climate_adaption_flattened, water_flattened, circular_economy_flattened, pollution_prevention_flattened, biodiversity_flattened], axis=0)\n",
    "\n",
    "with pd.ExcelWriter('../output/output_' + output_time + '.xlsx') as writer:\n",
    "    concatinated_taxonomy.to_excel(writer, \"Overall results\")\n",
    "    climate_mitigation_flattened.to_excel(writer, \"Climate mitigation\")\n",
    "    climate_adaption_flattened.to_excel(writer, \"Climate adaption\")\n",
    "    water_flattened.to_excel(writer, \"Water\")\n",
    "    circular_economy_flattened.to_excel(writer, \"Circular economy\")\n",
    "    pollution_prevention_flattened.to_excel(writer, \"Pollution prevention\")\n",
    "    biodiversity_flattened.to_excel(writer, \"Biodiversity\")\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('../output/output_raw_' + output_time + '.xlsx') as writer:\n",
    "    climate_mitigation_parsed.to_excel(writer, \"Climate mitigation\")\n",
    "    climate_adaption_parsed.to_excel(writer, \"Climate adaption\")\n",
    "    water_parsed.to_excel(writer, \"Water\")\n",
    "    circular_economy_parsed.to_excel(writer, \"Circular economy\")\n",
    "    pollution_prevention_parsed.to_excel(writer, \"Pollution prevention\")\n",
    "    biodiversity_parsed.to_excel(writer, \"Biodiversity\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
